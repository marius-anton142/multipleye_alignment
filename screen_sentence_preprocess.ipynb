{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q17gmm7G6VZX"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAf08EAe6WPg",
        "outputId": "fa3c744a-53c4-4e21-fa77-9e610cba263f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "params: ['en', 'ro', 'de'] data/dump\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "LANGS = [x.strip() for x in os.getenv(\"LANGS\", \"en,ro,de\").split(\",\") if x.strip()]\n",
        "OUT_DIR = os.getenv(\"OUT_DIR\", \"data/dump\").strip()\n",
        "\n",
        "print(\"params:\", LANGS, OUT_DIR)\n",
        "os.makedirs(OUT_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHhXRZOf7aIx"
      },
      "source": [
        "## Get spacy models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGNUWOHg7mKz",
        "outputId": "28f89975-e3eb-4577-8746-5d68272a6004"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xx-sent-ud-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/xx_sent_ud_sm-3.8.0/xx_sent_ud_sm-3.8.0-py3-none-any.whl (4.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xx-sent-ud-sm\n",
            "Successfully installed xx-sent-ud-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('xx_sent_ud_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "downloading en: en_core_web_lg\n",
            "Collecting en-core-web-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.7/400.7 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "downloading ro: ro_core_news_lg\n",
            "Collecting ro-core-news-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ro_core_news_lg-3.8.0/ro_core_news_lg-3.8.0-py3-none-any.whl (568.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m568.5/568.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ro-core-news-lg\n",
            "Successfully installed ro-core-news-lg-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ro_core_news_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "downloading de: de_core_news_lg\n",
            "Collecting de-core-news-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_lg-3.8.0/de_core_news_lg-3.8.0-py3-none-any.whl (567.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m567.8/567.8 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: de-core-news-lg\n",
            "Successfully installed de-core-news-lg-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download xx_sent_ud_sm\n",
        "\n",
        "language_models = {\n",
        "    \"en\": \"en_core_web_lg\",\n",
        "    \"ro\": \"ro_core_news_lg\",\n",
        "    \"de\": \"de_core_news_lg\",\n",
        "}\n",
        "\n",
        "for lm in LANGS:\n",
        "    if lm in language_models:\n",
        "        print(f\"downloading {lm}: {language_models[lm]}\")\n",
        "        !python -m spacy download {language_models[lm]}\n",
        "    else:\n",
        "        print(f\"use blank\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3J7nrCc7ebN"
      },
      "source": [
        "## Get the MultiplEYE json data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4L0uFaHSk-y",
        "outputId": "82215db6-c598-43ff-b3dc-6ec976f1db92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-21 21:29:48--  https://github.com/senisioi/repository/releases/download/eyelanguages0/languages_json_all.zip\n",
            "Resolving github.com (github.com)... 20.27.177.113\n",
            "Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/930203766/79211e09-0821-4137-a97f-9395779aa594?sp=r&sv=2018-11-09&sr=b&spr=https&se=2026-01-21T22%3A24%3A02Z&rscd=attachment%3B+filename%3Dlanguages_json_all.zip&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2026-01-21T21%3A23%3A34Z&ske=2026-01-21T22%3A24%3A02Z&sks=b&skv=2018-11-09&sig=Zx2PKJ8OzHuShDFxhcRTq0kYEutl60vL4kTE4PRB3N4%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2OTAzMTI4OCwibmJmIjoxNzY5MDMwOTg4LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.jjWlgaTiogP6Y8Q2ghcfU6cjqK0c3LbQF2-Jo4OPG28&response-content-disposition=attachment%3B%20filename%3Dlanguages_json_all.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2026-01-21 21:29:48--  https://release-assets.githubusercontent.com/github-production-release-asset/930203766/79211e09-0821-4137-a97f-9395779aa594?sp=r&sv=2018-11-09&sr=b&spr=https&se=2026-01-21T22%3A24%3A02Z&rscd=attachment%3B+filename%3Dlanguages_json_all.zip&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2026-01-21T21%3A23%3A34Z&ske=2026-01-21T22%3A24%3A02Z&sks=b&skv=2018-11-09&sig=Zx2PKJ8OzHuShDFxhcRTq0kYEutl60vL4kTE4PRB3N4%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2OTAzMTI4OCwibmJmIjoxNzY5MDMwOTg4LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.jjWlgaTiogP6Y8Q2ghcfU6cjqK0c3LbQF2-Jo4OPG28&response-content-disposition=attachment%3B%20filename%3Dlanguages_json_all.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 578999 (565K) [application/octet-stream]\n",
            "Saving to: ‘languages_json_all.zip’\n",
            "\n",
            "languages_json_all. 100%[===================>] 565.43K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2026-01-21 21:29:49 (4.17 MB/s) - ‘languages_json_all.zip’ saved [578999/578999]\n",
            "\n",
            "Archive:  languages_json_all.zip\n",
            "   creating: languages_json/\n",
            "  inflating: languages_json/multipleye_stimuli_experiment_ca.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_en.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_mk.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_pt.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_sl.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_lt.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_es.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_lv.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_nl.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_fr.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_rm.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_yue.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_pl.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_el.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_kl.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_sq.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_hi.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_eu.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_cs.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_et.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_zh.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_ro.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_ru.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_ar.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_it.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_hr.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_zd.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_sv.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_uk.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_de.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_tr.json  \n",
            "  inflating: languages_json/multipleye_stimuli_experiment_ha.json  \n"
          ]
        }
      ],
      "source": [
        "! rm -rf languages*\n",
        "! wget https://github.com/senisioi/repository/releases/download/eyelanguages0/languages_json_all.zip\n",
        "! unzip languages_json_all.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0xc3i_xvQU1Z"
      },
      "outputs": [],
      "source": [
        "SPACY_LANGUAGES = [\"ca\", \"de\", \"el\", \"en\", \"es\", \"fr\", \"hr\", \"it\", \"lt\", \"mk\", \"nl\", \"pl\", \"pt\", \"ro\", \"ru\", \"sl\", \"sv\", \"uk\", \"zh\"]\n",
        "\n",
        "CODE2LANG = {\n",
        "    \"ar\": \"Arabic\",\n",
        "    \"ca\": \"Catalan\",\n",
        "    \"cs\": \"Czech\",\n",
        "    \"de\": \"German\",\n",
        "    \"gsw\": \"Swiss German\",\n",
        "    \"el\": \"Greek\",\n",
        "    \"en\": \"English\",\n",
        "    #\"es\": \"Spanish\",\n",
        "    \"et\": \"Estonian\",\n",
        "    \"eu\": \"Basque\",\n",
        "    #\"fr\": \"French\",\n",
        "    #\"he\": \"Hebrew\",\n",
        "    \"hi\": \"Hindi\",\n",
        "    \"hr\": \"Croatian\",\n",
        "    \"it\": \"Italian\",\n",
        "    \"kl\": \"Kalaallisut\",\n",
        "    \"lt\": \"Lithuanian\",\n",
        "    \"lv\": \"Latvian\",\n",
        "    \"mk\": \"Macedonian\",\n",
        "    \"nl\": \"Dutch\",\n",
        "    \"pl\": \"Polish\",\n",
        "    \"pt\": \"Portuguese\",\n",
        "    \"rm\": \"Romansh\",\n",
        "    \"ro\": \"Romanian\",\n",
        "    \"ru\": \"Russian\",\n",
        "    \"sl\": \"Slovenian\",\n",
        "    \"sq\": \"Albanian\",\n",
        "    \"sv\": \"Swedish\",\n",
        "    \"tr\": \"Turkish\",\n",
        "    \"uk\": \"Ukrainian\",\n",
        "    #\"yue\": \"Cantonese\",\n",
        "    \"zh\": \"Chinese\"\n",
        "}\n",
        "\n",
        "LANGUAGES = list(CODE2LANG.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDPp5lF77uB2"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "R27JzBYj9iIw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import spacy\n",
        "\n",
        "def load_all_json(lang_folder):\n",
        "    all_data = {}\n",
        "    for file in os.listdir(lang_folder):\n",
        "        if file.endswith('.json'):\n",
        "            lang_code = file.replace('.json', '').replace('multipleye_stimuli_experiment_', '')\n",
        "            if lang_code == 'zd':\n",
        "                lang_code = 'gsw'\n",
        "            if (lang_code not in LANGUAGES) or (lang_code not in LANGS):\n",
        "                continue\n",
        "            with open(os.path.join(lang_folder, file), 'r', encoding='utf-8') as f:\n",
        "                all_data[lang_code] = json.load(f)\n",
        "    return all_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqdZjNWw9i6C",
        "outputId": "f95b3527-c1e9-4696-dcd9-0f79d71c9ef6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en {'stimulus_id': 1, 'stimulus_name': 'PopSci_MultiplEYE', 'stimulus_type': 'experiment', 'pages': ['The MultiplEYE Project\\n\\nThe name \"MultiplEYE\" is a wordplay combining \"multilingualism\" or \"multiple languages\" with \"eye\" from \"eye-tracking\". MultiplEYE is a COST Action funded by the European Union. COST Actions are research networks supported by the European Cooperation in Science and Technology or COST for short. As a funding organisation, COST supports our growing network of researchers across Europe and beyond by providing financial assistance for conducting different networking activities.', 'These activities include working group meetings, training schools to share skills with younger researchers, and scientific research visits. The project title of the MultiplEYE COST Action is: Enabling multilingual eye-tracking data collection for human and machine language processing research. This means that the MultiplEYE COST Action aims to foster an interdisciplinary network of research groups working on collecting eye-tracking data from reading in multiple languages.', 'The goal is to support the development of a large multilingual eye-tracking corpus and enable researchers to collect data by sharing their knowledge between various fields, including linguistics, psychology, speech and language pathology, and computer science. This data collection can then be used to study human language processing from a psycholinguistic perspective as well as to improve and evaluate computational language processing from a machine-learning perspective.', 'What is \"eye-tracking\"?\\nEye-tracking is the process of measuring the point of gaze - where you are looking - and the movements of the eyes between fixed points of gaze. The device used to measure the eye positions and eye movements is called an eye-tracker. It consists of an infrared camera, using a light frequency that does not bother or hurt the human eye.', \"With the help of image recognition algorithms, the eye-tracker can estimate gaze points very accurately by knowing the position of the head and eyes, the distance to the screen a participant is looking at and the eye-tracker's position. Eye-tracking is a helpful technology for many applications. For example, it can help detect tiredness while driving or it can support applications for screening and training purposes in the medical domain. Eye-tracking is also used in gaming, marketing, and human-computer interaction.\", \"Why is eye-tracking while reading especially interesting for our project?\\nAs you read these words, the eye-tracker follows your eye's movements over the text. This provides information about how long you spend looking at a text, or more specifically, how long you spent on each word, which words you skipped, which words you dwelled on, and whether you had to go back and reread parts of the text to understand it better.\", \"As your brain is processing the content of the text, your eye movements reflect a lot of the linguistic and cognitive processing going on almost in real time. Thus, the recorded data is a gold mine of information about how we put together a text's meaning and grammatical structures. It shows which parts of the text we struggle with and which parts are easily readable. It is up to the researchers to later explain which linguistic factors caused which type of eye movements.\", 'The motivation behind MultiplEYE is that eye-tracking data is still sparse, especially for languages with fewer speakers. Such extensive data collection is a challenge in terms of developing and agreeing on the experimental design, the complexity, and the types of texts to be read by the participants. Other decisions that seem less relevant but are, in fact, very important include the font type and size the text is presented in, the order of the texts, the experiment procedure, and how the data will be processed.', 'But once completed, this dataset will allow us to investigate many topics related to psycholinguistics and computational linguistics. For example, we can compare the reading behaviour across different languages. Does the scripture, for example, the Latin alphabet versus Cyrillic or Arabic scripts, impact reading times? An example concerning the computational processing of text could involve using eye-tracking data to advance artificial intelligence applications that imitate the human reading process. This could be used to build better machine translation systems or to improve the automatic extraction of keywords from the text.', 'Receiving eye-tracking data from many participants, including yourself, by reading texts in many different languages will be a great foundation for our research. It will be the main factor in turning our research network into a successful endeavour. We hope to clear the way for advancing research in various subfields of linguistics by supporting and connecting a large group of researchers.', 'The main outcomes of the MultiplEYE Action will be a large dataset containing eye-tracking data in many languages and a platform for new collaborations building on this type of data. If you are reading this text, you are already supporting our cause by allowing us to collect and analyse your eye movements while reading and comprehending language. Thank you!']}\n",
            "ro {'stimulus_id': 1, 'stimulus_name': 'PopSci_MultiplEYE', 'stimulus_type': 'experiment', 'pages': ['Proiectul MultiplEYE\\n\\nNumele „MultiplEYE” este un joc de cuvinte care combină „multilingvism” sau „limbi multiple” cu „eye” (ochi) din „eye-tracking” (urmărire oculară). MultiplEYE este o Acțiune COST finanțată de Uniunea Europeană. Acțiunile COST sunt rețele de cercetare sprijinite de Cooperarea Europeană în Știință și Tehnologie, pe scurt COST. Ca organizație de finanțare, COST susține rețeaua noastră în creștere de cercetători din Europa și din afara ei, oferind sprijin financiar pentru desfășurarea diverselor activități de networking.', 'Aceste activități includ întâlniri ale grupurilor de lucru, școli de formare pentru a împărtăși abilități cu cercetătorii mai tineri și vizite științifice de cercetare. Titlul proiectului Acțiunii COST MultiplEYE este: Facilitarea colectării datelor de urmărire oculară în mai multe limbi pentru cercetarea procesării limbajului de către om și a procesării automate. Acest lucru înseamnă că Acțiunea COST MultiplEYE urmărește să încurajeze o rețea interdisciplinară de grupuri de cercetare care lucrează la colectarea datelor de urmărire oculară în timpul citirii în mai multe limbi.', 'Obiectivul este de a sprijini dezvoltarea unui corpus multilingv extins de urmărire oculară și de a permite cercetătorilor să colecteze date prin împărtășirea cunoștințelor între diverse domenii, inclusiv lingvistică, psihologie, logopedie și informatică. Aceste date pot fi apoi utilizate pentru a studia procesarea limbajului uman dintr-o perspectivă psiholingvistică, precum și pentru a îmbunătăți și evalua procesarea computațională a limbajului dintr-o perspectivă de învățare automată.', 'Ce este urmărirea oculară?\\nUrmărirea oculară este procesul de măsurare a punctului de privire – locul unde te uiți – și a mișcărilor ochilor între punctele fixe de privire. Dispozitivul utilizat pentru a măsura pozițiile și mișcările ochilor se numește eye-tracker. Acesta constă într-o cameră cu infraroșu, folosind o frecvență de lumină care nu deranjează sau rănește ochiul uman.', 'Cu ajutorul algoritmilor de recunoaștere a imaginii, eye-tracker-ul poate estima punctele de privire cu mare precizie, cunoscând poziția capului și a ochilor, distanța față de ecranul la care se uită participantul și poziția dispozitivului. Urmărirea oculară este o tehnologie utilă pentru multe aplicații. De exemplu, poate ajuta la detectarea oboselii în timpul condusului sau poate sprijini aplicațiile pentru screening și instruire în domeniul medical. Urmărirea oculară este folosită, de asemenea, în gaming, marketing și interacțiunea om-calculator.', 'De ce urmărirea oculară în timpul citirii prezintă interes pentru proiectul nostru?\\nÎn timp ce citești aceste cuvinte, eye-tracker-ul urmărește mișcările ochilor tăi pe text. Acest lucru oferă informații despre cât timp petreci uitându-te la un text sau, mai specific, cât timp ai petrecut pe fiecare cuvânt, ce cuvinte ai sărit, asupra căror cuvinte te-ai oprit și dacă a trebuit să revii și să recitești părți ale textului pentru a-l înțelege mai bine.', 'Pe măsură ce creierul tău procesează conținutul textului, mișcările ochilor reflectă multe dintre procesele lingvistice și cognitive care au loc aproape în timp real. Astfel, datele înregistrate sunt o adevărată comoară de informații despre cum construim sensul și structurile gramaticale ale unui text. Ele arată cu care părți ale textului întâmpinăm dificultăți și care sunt ușor de citit. Este responsabilitatea cercetătorilor să explice ulterior ce factori lingvistici au cauzat diferitele tipuri de mișcări ale ochilor.', 'Motivația din spatele MultiplEYE este că datele de urmărire oculară sunt încă puține, mai ales pentru limbile cu un număr mai mic de vorbitori. O astfel de colectare extinsă a datelor reprezintă o provocare în ceea ce privește dezvoltarea și convenirea asupra designului experimental, complexității și tipurilor de texte care urmează să fie citite de către participanți. Alte decizii care par mai puțin relevante, dar sunt de fapt foarte importante, includ tipul și dimensiunea fontului în care este prezentat textul, ordinea textelor, procedura experimentului și modul în care vor fi procesate datele.', 'Dar, odată finalizat, acest set de date ne va permite să investigăm multe subiecte legate de psiholingvistică și lingvistică computațională. De exemplu, putem compara comportamentul de citire între diferite limbi. Influențează oare tipul de scriere, de exemplu, alfabetul latin față de scrierea chirilică sau arabă, timpii de citire? Un exemplu legat de procesarea computațională a textului ar putea implica utilizarea datelor de urmărire oculară pentru a îmbunătăți aplicațiile de inteligență artificială care imită procesul de citire uman. Acest lucru ar putea fi folosit pentru a construi sisteme mai bune de traducere automată sau pentru a îmbunătăți extragerea automată a cuvintelor cheie din text.', 'Primirea datelor de urmărire oculară de la mulți participanți, inclusiv de la tine, prin citirea textelor în multe limbi diferite, va constitui o bază excelentă pentru cercetarea noastră. Aceasta va fi factorul principal care va transforma rețeaua noastră de cercetare într-un demers de succes. Sperăm să deschidem calea pentru avansarea cercetării în diverse subdomenii ale lingvisticii prin sprijinirea și conectarea unui grup mare de cercetători.', 'Principalele rezultate ale Acțiunii MultiplEYE vor fi un set mare de date care conține date de urmărire oculară în multe limbi și o platformă pentru noi colaborări care se bazează pe acest tip de date. Dacă citești acest text, deja ne sprijini cauza, permițându-ne să colectăm și să analizăm mișcările ochilor tăi în timp ce citești și înțelegi limbajul. Îți mulțumim!']}\n",
            "de {'stimulus_id': 1, 'stimulus_name': 'PopSci_MultiplEYE', 'stimulus_type': 'experiment', 'pages': ['Das MultiplEYE Projekt\\n\\nDer Name „MultiplEYE“ ist ein Wortspiel, das „multilingualism“ (Englisch: Mehrsprachigkeit) oder „multiple languages“ (Englisch: mehrere Sprachen) mit „eye“ (Englisch: Auge) aus „Eye-Tracking“ (Englisch: Blickverfolgung) verbindet. MultiplEYE ist eine von der Europäischen Union finanzierte COST Action. COST Actions sind Forschungsnetzwerke, die von der European Cooperation in Science and Technology, oder kurz COST, unterstützt werden.', 'Als Förderorganisation unterstützt COST unser ständig wachsendes Netzwerk von Forschern in ganz Europa und darüber hinaus, indem es finanzielle Mittel für die Durchführung verschiedener Arten von Networking-Aktivitäten bereitstellt. Zu diesen Aktivitäten gehören Arbeitsgruppentreffen, Weiterbildungskurse, um Expertise an jüngeren Forscher weiterzugeben, und wissenschaftliche Forschungsaufenthalte.', 'Der Projekttitel der MultiplEYE COST Action lautet: Ermöglichung der mehrsprachigen Eye-Tracking-Datenerfassung für die menschliche und maschinelle Sprachverarbeitungsforschung (übersetzt aus dem Englischen). Das bedeutet, dass die MultiplEYE COST Action darauf abzielt, ein interdisziplinäres Netzwerk von Forschungsgruppen zu fördern, die an der Erhebung von Augenbewegungsdaten beim Lesen in verschiedenen Sprachen arbeiten.', 'Ziel ist es, die Entwicklung eines großen mehrsprachigen Korpus von Augenbewegungsdaten zu unterstützen und es Forschern zu ermöglichen, Daten zu sammeln, indem sie ihr Expertenwissen aus verschiedenen Disziplinen teilen. Dazu gehören Linguistik, Psychologie, Sprach- und Lesetherapie und Informatik. Diese Datensammlung kann dann verwendet werden, um die menschliche Sprachverarbeitung aus psycholinguistischer Perspektive zu untersuchen, sowie die computergestützte Sprachverarbeitung aus der Perspektive des Maschinellen Lernens zu verbessern und zu evaluieren.', 'Was ist „Eye-Tracking”?\\nEye-Tracking ist der Prozess, bei dem der Blickpunkt – wohin Sie schauen – und die Augenbewegungen zwischen festen Blickpunkten gemessen werden. Das Gerät, mit dem die Augenpositionen und -bewegungen gemessen werden, wird als Eye-Tracker bezeichnet. Es besteht aus einer Infrarotkamera, die eine Lichtfrequenz verwendet, die das menschliche Auge nicht stört oder verletzt.', 'Mit Hilfe von Bilderkennungsalgorithmen ist der Eye-Tracker in der Lage, Blickpunkte sehr genau zu schätzen, indem er die Position von Kopf und Augen, die Entfernung zum Bildschirm, auf den ein Teilnehmer blickt, und die Position des Eye-Trackers kennt.\\nEye-Tracking ist eine nützliche Technologie für eine Vielzahl von Anwendungen. Beispielsweise kann es dabei helfen, Müdigkeit beim Autofahren zu erkennen oder Anwendungen für Screening- und Trainingszwecke im medizinischen Bereich unterstützen. Eye-Tracking wird auch in der Gaming-Branche, im Marketing und in der Forschung zur Interaktion von Mensch und Maschine eingesetzt.', 'Warum ist Eye-Tracking beim Lesen für unser Projekt besonders interessant?\\nWährend Sie diese Wörter lesen, folgt der Eye-Tracker Ihren Augenbewegungen über den Text. Dies gibt Aufschluss darüber, wie lange Sie sich einen Text angesehen haben, oder genauer gesagt, wie lange Sie zum Lesen jedes Wortes gebraucht haben, welche Wörter Sie übersprungen haben, bei welchen Wörtern Sie verweilt sind und ob Sie zurückgehen mussten, um die bereits gelesenen Wörter nochmals anzuschauen, um Teile des Textes besser zu verstehen.', 'Während Ihr Gehirn den Inhalt des Textes verarbeitet, spiegeln Ihre Augenbewegungen fast in Echtzeit einen Großteil der dafür benötigten sprachlichen und kognitiven Verarbeitung wider. Die aufgezeichneten Daten sind also eine Goldgrube an Informationen darüber, wie wir die Bedeutung und die grammatikalischen Strukturen eines Textes zusammensetzen. Es zeigt, mit welchen Textstellen wir zu kämpfen haben und welche einfach lesbar sind. Die Forscher können daraus ableiten, welche sprachlichen Faktoren welche Art von Augenbewegungen verursacht haben.', 'Die Motivation hinter MultiplEYE ist, dass Eye-Tracking-Daten noch immer nur spärlich verfügbar sind, insbesondere für  Sprachen mit weniger Sprechern. Eine so große Datensammlung ist eine Herausforderung in Bezug auf die Entwicklung und die Vereinbarung des experimentellen Designs, die Komplexität und die Art der Texte, die von den Teilnehmern gelesen werden sollen. Andere Entscheidungen, die weniger relevant erscheinen, aber tatsächlich sehr wichtig sind, betreffen die Schriftart und -größe des Textes, die Reihenfolge der Texte, den Versuchsablauf und die Art und Weise, wie die Daten danach verarbeitet werden.', 'Aber sobald dieser Datensatz fertig ist, wird er es uns ermöglichen, viele Themen im Zusammenhang mit Psycholinguistik und Computerlinguistik zu untersuchen. Beispielsweise können wir das Leseverhalten über verschiedene Sprachen hinweg vergleichen. Hat die Schrift, z.B. das lateinische Alphabet im Vergleich zur kyrillischen oder arabischen Schrift, einen Einfluss auf die Lesezeit?', 'Ein Beispiel für die computergestützte Textverarbeitung könnte die Verwendung von Eye-Tracking-Daten sein, um Anwendungen künstlicher Intelligenz voranzutreiben, die den menschlichen Leseprozess nachahmen. Diese Erkenntnisse könnten verwendet werden, um bessere maschinelle Übersetzungssysteme zu bauen oder um die automatische Extraktion von Schlüsselwörtern aus Texten zu verbessern.', 'Das Erhalten von Eye-Tracking-Daten von vielen verschiedenen Teilnehmern, einschließlich Ihnen selbst, die Texte in vielen verschiedenen Sprachen lesen, wird eine großartige Grundlage für unsere Forschung sein. Sie wird der Hauptfaktor sein, um unser Forschungsnetzwerk zu einem erfolgreichen Unterfangen zu machen.\\nWir hoffen, dass wir den Weg für die Förderung der Forschung in verschiedenen Teilgebieten der Linguistik ebnen können, indem wir eine große Gruppe von Forschern unterstützen und vernetzen.', 'Die Hauptergebnisse der MultiplEYE Action werden ein großer Datensatz mit Blickverfolgungsdaten in vielen Sprachen und eine Plattform für neue Kooperationen sein, die diese Art von Daten nutzen.\\nWenn Sie diesen Text lesen, unterstützen Sie uns bereits, indem Sie uns erlauben, Ihre Augenbewegungen beim Lesen und Verstehen von Sprache zu erfassen und zu analysieren. Vielen Dank!']}\n"
          ]
        }
      ],
      "source": [
        "all_data = load_all_json('languages_json')\n",
        "for k,v in all_data.items():\n",
        "  print(k, v[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5Z0HnuEDdxt"
      },
      "source": [
        "## Prepare spaCy code to generate template csv files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Hnvn1bCFWLF_"
      },
      "outputs": [],
      "source": [
        "LANG_FOLDER = \"languages_json\"\n",
        "NLP_MODEL = None\n",
        "CURRENT_LANG = ''\n",
        "IN_DIR = 'languages_json/'\n",
        "\n",
        "from spacy.util import get_lang_class\n",
        "\n",
        "\n",
        "def exists_spacy_blank(lang_code):\n",
        "    try:\n",
        "        get_lang_class(lang_code)\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def load_spacy_model(lang_code, small=True):\n",
        "    model = None\n",
        "    if lang_code in SPACY_LANGUAGES:\n",
        "        genre = 'news'\n",
        "        if lang_code in {'zh', 'en'}:\n",
        "            genre = 'web'\n",
        "        if lang_code == 'rm':\n",
        "            return ''\n",
        "        model_name = f'{lang_code}_core_{genre}_{\"sm\" if small else \"lg\"}'\n",
        "        print(f\"Loading model {model_name} for {lang_code}\")\n",
        "        model = spacy.load(model_name)\n",
        "        model.add_pipe(\"sentencizer\")\n",
        "    elif lang_code == \"rm\":\n",
        "        model = spacy.load(\"it_core_news_lg\")\n",
        "        # keep 'morphologizer' ?\n",
        "        model.disable_pipes('tok2vec', 'tagger', 'parser', 'lemmatizer', 'attribute_ruler', 'ner')\n",
        "    elif lang_code == 'gsw':\n",
        "        model = spacy.load('de_core_news_lg')\n",
        "    elif exists_spacy_blank(lang_code):\n",
        "        print(f\"Loading model blank model for {lang_code}\")\n",
        "        model = spacy.blank(lang_code)\n",
        "        model.add_pipe(\"sentencizer\")\n",
        "    else:\n",
        "        model_name = f'xx_sent_ud_sm'\n",
        "        print(f\"Loading model {model_name} for {lang_code}\")\n",
        "        model = spacy.load(model_name)\n",
        "        model.add_pipe(\"sentencizer\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_nlp(lang_code, small=False):\n",
        "    \"\"\"To avoid loading all models at the same time\n",
        "    \"\"\"\n",
        "    global NLP_MODEL, CURRENT_LANG\n",
        "    if lang_code != CURRENT_LANG:\n",
        "        try:\n",
        "            print(f\"Deleting model for {CURRENT_LANG}\")\n",
        "            del NLP_MODEL\n",
        "        except:\n",
        "            print(\"No model to delete\")\n",
        "        print(f\"Loading model for {lang_code}\")\n",
        "        NLP_MODEL = load_spacy_model(lang_code, small=small)\n",
        "        CURRENT_LANG = lang_code\n",
        "    return NLP_MODEL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "13jbiTKgbx4O"
      },
      "outputs": [],
      "source": [
        "def feats_str(token):\n",
        "    if not token.morph:\n",
        "        return \"_\"\n",
        "    md = token.morph.to_dict()\n",
        "    if not md:\n",
        "        return \"_\"\n",
        "    bits = []\n",
        "    for k in sorted(md):\n",
        "        v = md[k]\n",
        "        if isinstance(v, (list, tuple)):\n",
        "            bits.append(f\"{k}={','.join(v)}\")\n",
        "        else:\n",
        "            bits.append(f\"{k}={v}\")\n",
        "    return \"|\".join(bits) if bits else \"_\"\n",
        "\n",
        "\n",
        "def get_head(token, sent):\n",
        "    if token.head == token or token.dep_ == \"ROOT\":\n",
        "        head = 0\n",
        "        deprel = \"root\"\n",
        "    else:\n",
        "        head = (token.head.i - sent.start) + 1  # 1-based in sentence\n",
        "        deprel = token.dep_.lower() if token.dep_ else \"_\"\n",
        "    return head, deprel\n",
        "\n",
        "\n",
        "def get_misc(token, include_ner=True):\n",
        "    misc_parts = []\n",
        "    if not token.whitespace_:\n",
        "        misc_parts.append(\"SpaceAfter=No\")\n",
        "    if include_ner and token.ent_iob_ != \"O\":\n",
        "        misc_parts.append(f\"NER={token.ent_iob_}-{token.ent_type_}\")\n",
        "    misc = \"|\".join(misc_parts) if misc_parts else \"_\"\n",
        "    return misc\n",
        "\n",
        "\n",
        "def iter_pages(stimuli, nlp):\n",
        "    for stim in stimuli:\n",
        "        sid, sname = stim[\"stimulus_id\"], stim[\"stimulus_name\"]\n",
        "        for pnum, page_text in enumerate(stim[\"pages\"], start=1):\n",
        "            yield sid, sname, pnum, nlp(page_text)\n",
        "\n",
        "def stimuli2csv(stimuli, lang_code, level=\"page\", small=False):\n",
        "    rows = []\n",
        "    nlp = get_nlp(lang_code, small=small)\n",
        "    for sid, sname, page, doc in iter_pages(stimuli, nlp):\n",
        "        ptext = doc.text\n",
        "        document = nlp(ptext)\n",
        "        for sent_idx, sentence in enumerate(document.sents):\n",
        "            eos = {\n",
        "              \"language\": CODE2LANG[lang_code],\n",
        "              \"language_code\": lang_code,\n",
        "              \"stimulus_name\": sname,\n",
        "              \"page\": page,\n",
        "              #\"sent_idx\": sent_idx+1,\n",
        "              \"token\": \"<eos>\",\n",
        "              \"is_alpha\": False,\n",
        "              \"is_stop\": False,\n",
        "              \"is_punct\": False,\n",
        "              \"lemma\": \"\",\n",
        "              \"upos\": \"\",\n",
        "              \"xpos\": \"\",\n",
        "              \"feats\": \"\",\n",
        "              \"head\": \"\",\n",
        "              \"deprel\": \"\",\n",
        "              \"deps\": \"\",\n",
        "              \"misc\": \"\"\n",
        "              }\n",
        "            for token in sentence:\n",
        "                head, deprel = get_head(token, sentence)\n",
        "                rows.append(\n",
        "                    {\n",
        "                        #\"stimulus_id\": sid,\n",
        "                        \"language\": CODE2LANG[lang_code],\n",
        "                        \"language_code\": lang_code,\n",
        "                        \"stimulus_name\": sname,\n",
        "                        \"page\": page,\n",
        "                        #\"sent_idx\": sent_idx+1,\n",
        "                        \"token\": token.text,\n",
        "                        \"is_alpha\": token.is_alpha,\n",
        "                        \"is_stop\": token.is_stop,\n",
        "                        \"is_punct\": token.is_punct,\n",
        "                        \"lemma\": token.lemma_,\n",
        "                        \"upos\": token.pos_,\n",
        "                        \"xpos\": token.tag_,\n",
        "                        \"feats\": feats_str(token),\n",
        "                        \"head\": head,\n",
        "                        \"deprel\": deprel,\n",
        "                        \"deps\": \"_\",\n",
        "                        \"misc\": get_misc(token, include_ner=True)\n",
        "                    }\n",
        "                )\n",
        "            rows.append(eos)\n",
        "\n",
        "\n",
        "    df = pd.DataFrame(rows).sort_values(by=[\"stimulus_name\", \"page\"])\n",
        "    df = pd.DataFrame(rows)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KE4-VqxpOrt_"
      },
      "source": [
        "## Generate csv templates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMb69Drkbx7D",
        "outputId": "e6047f39-13c3-4dab-c187-acaacb3759e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleting model for \n",
            "Loading model for en\n",
            "Loading model en_core_web_lg for en\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 1/3 [00:09<00:19,  9.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleting model for en\n",
            "Loading model for ro\n",
            "Loading model ro_core_news_lg for ro\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 2/3 [00:18<00:08,  8.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleting model for ro\n",
            "Loading model for de\n",
            "Loading model de_core_news_lg for de\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:31<00:00, 10.41s/it]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "preproc = defaultdict(dict)\n",
        "for lang_code, data in tqdm(all_data.items()):\n",
        "    if lang_code not in LANGS:\n",
        "        continue\n",
        "    preproc[lang_code] = stimuli2csv(data, lang_code, small=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "def stimuli2sentences(stimuli, lang_code, small=False):\n",
        "    rows = []\n",
        "    nlp = get_nlp(lang_code, small=small)\n",
        "\n",
        "    global_sent_idx = 0\n",
        "\n",
        "    for sid, sname, page, doc in iter_pages(stimuli, nlp):\n",
        "        for sent in doc.sents:\n",
        "            raw_text = sent.text\n",
        "\n",
        "            if '\\n' in raw_text:\n",
        "                parts = [t.strip() for t in raw_text.split('\\n') if t.strip()]\n",
        "            else:\n",
        "                cleaned = raw_text.strip()\n",
        "                parts = [cleaned] if cleaned else []\n",
        "\n",
        "            for part in parts:\n",
        "                rows.append({\n",
        "                    \"stimulus_id\": sid,\n",
        "                    \"stimulus_name\": sname,\n",
        "                    \"language\": lang_code,\n",
        "                    \"screen_id\": page,\n",
        "                    \"global_sent_index\": global_sent_idx,\n",
        "                    \"text\": part\n",
        "                })\n",
        "                global_sent_idx += 1\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "def download_processed_data(all_data, languages=['en', 'ro', 'de']):\n",
        "    out_dir = \"processed_sentences\"\n",
        "    if os.path.exists(out_dir):\n",
        "        shutil.rmtree(out_dir)\n",
        "    os.makedirs(out_dir)\n",
        "\n",
        "    for lang in languages:\n",
        "        if lang not in all_data:\n",
        "            continue\n",
        "\n",
        "        print(f\"Processing {lang}...\")\n",
        "        df = stimuli2sentences(all_data[lang], lang)\n",
        "\n",
        "        filename = f\"{out_dir}/{lang}_sentences.csv\"\n",
        "        df.to_csv(filename, index=False)\n",
        "        print(f\"Saved: {filename} ({len(df)} sentences)\")\n",
        "\n",
        "    print(\"Zipping files...\")\n",
        "    shutil.make_archive(\"eyetracking_sentences\", 'zip', out_dir)\n",
        "    files.download(\"eyetracking_sentences.zip\")\n",
        "\n",
        "download_processed_data(all_data, languages=['en', 'ro', 'de'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "IHzDZaXlJ84A",
        "outputId": "62855e33-1b74-4908-e6bb-de6c303ad828"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing en...\n",
            "Deleting model for de\n",
            "Loading model for en\n",
            "Loading model en_core_web_lg for en\n",
            "Saved: processed_sentences/en_sentences.csv (312 sentences)\n",
            "Processing ro...\n",
            "Deleting model for en\n",
            "Loading model for ro\n",
            "Loading model ro_core_news_lg for ro\n",
            "Saved: processed_sentences/ro_sentences.csv (327 sentences)\n",
            "Processing de...\n",
            "Deleting model for ro\n",
            "Loading model for de\n",
            "Loading model de_core_news_lg for de\n",
            "Saved: processed_sentences/de_sentences.csv (342 sentences)\n",
            "Zipping files...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0f646e32-a134-4026-8196-5539c537e7a4\", \"eyetracking_sentences.zip\", 56847)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}